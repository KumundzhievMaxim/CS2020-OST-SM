{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inquirer in /opt/conda/lib/python3.8/site-packages (2.7.0)\n",
      "Requirement already satisfied: blessed==1.17.6 in /opt/conda/lib/python3.8/site-packages (from inquirer) (1.17.6)\n",
      "Requirement already satisfied: readchar==2.0.1 in /opt/conda/lib/python3.8/site-packages (from inquirer) (2.0.1)\n",
      "Requirement already satisfied: python-editor==1.0.4 in /opt/conda/lib/python3.8/site-packages (from inquirer) (1.0.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from blessed==1.17.6->inquirer) (1.15.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.8/site-packages (from blessed==1.17.6->inquirer) (0.2.5)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "!pip install inquirer\n",
    "import inquirer\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SparkFactory:\n",
    "    def __init__(self):\n",
    "        self.source_dataset_folder = Path('./dataset/')\n",
    "        self.spark = SparkSession.builder.appName(__name__).getOrCreate()\n",
    "    \n",
    "    def validate_available_dataset(self):\n",
    "        return [name for name in glob.glob(f'{self.source_dataset_folder}/**/*.csv')]\n",
    "\n",
    "    def read_dataset(self, dataset):\n",
    "        \"\"\"Read source dataset with sql interface\"\"\"\n",
    "        print(f'Reading dataset: {dataset}')\n",
    "        try:\n",
    "            self.spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(dataset) \n",
    "            data = self.spark.read.option(\"header\", \"true\").csv(dataset)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def _one_hot_endcoder(data):\n",
    "        \n",
    "        Vectorizer = CountVectorizer(inputCol=\"Color_Array\", outputCol=\"Color_OneHotEncoded\", vocabSize=4, minDF=1.0)\n",
    "        return data\n",
    "    \n",
    "    def run(self):\n",
    "        # validate and choose dataset to work with\n",
    "        assert input('Hi! Would u like to see available datasets to process? Y/n: ') == 'Y', 'Okay, see u later!'    \n",
    "        print(self.validate_available_dataset())\n",
    "        dataset = input('Good! Choose one of datasets to work with pasting full path shown in previous step: ')\n",
    "        \n",
    "        while False:\n",
    "            assert Path(dataset).is_file() == True, 'Wrong path, try one more time!'\n",
    "        print(f'Nice, we gonna to preprocess {dataset}')\n",
    "        data = self.read_dataset(dataset)\n",
    "        # print(data.toPandas().head(10))\n",
    "        \n",
    "        # validate and choose method to use\n",
    "        print('Available methods to apply: [OHE, B, C, D]')\n",
    "        method = input('Now choose one of methods to apply ')\n",
    "        if method == 'OHE':\n",
    "            data = _one_hot_endcoder(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset: dataset/CICIDS2017_fine/X_train.csv\n"
     ]
    }
   ],
   "source": [
    "# SparkFactory().run()\n",
    "data = SparkFactory().read_dataset('dataset/CICIDS2017_fine/X_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "categorical_columns = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "numerical_columns = [f.name for f in data.schema.fields]\n",
    "print(len(categorical_columns))\n",
    "print(len(numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'StringType': ['_c0',\n",
       "              'ack_psh_rst_syn_fin_cnt_0',\n",
       "              'ack_psh_rst_syn_fin_cnt_1',\n",
       "              'ack_psh_rst_syn_fin_cnt_2',\n",
       "              'ack_psh_rst_syn_fin_cnt_3',\n",
       "              'ack_psh_rst_syn_fin_cnt_4',\n",
       "              'bytes_in',\n",
       "              'bytes_out',\n",
       "              'dst_port',\n",
       "              'hdr_bin_40',\n",
       "              'hdr_ccnt_0',\n",
       "              'hdr_ccnt_1',\n",
       "              'hdr_ccnt_2',\n",
       "              'hdr_ccnt_3',\n",
       "              'hdr_ccnt_4',\n",
       "              'hdr_ccnt_5',\n",
       "              'hdr_ccnt_6',\n",
       "              'hdr_ccnt_7',\n",
       "              'hdr_ccnt_8',\n",
       "              'hdr_ccnt_9',\n",
       "              'hdr_ccnt_10',\n",
       "              'hdr_ccnt_11',\n",
       "              'hdr_distinct',\n",
       "              'hdr_mean',\n",
       "              'intervals_ccnt_0',\n",
       "              'intervals_ccnt_1',\n",
       "              'intervals_ccnt_2',\n",
       "              'intervals_ccnt_3',\n",
       "              'intervals_ccnt_4',\n",
       "              'intervals_ccnt_5',\n",
       "              'intervals_ccnt_6',\n",
       "              'intervals_ccnt_7',\n",
       "              'intervals_ccnt_8',\n",
       "              'intervals_ccnt_9',\n",
       "              'intervals_ccnt_10',\n",
       "              'intervals_ccnt_11',\n",
       "              'intervals_ccnt_12',\n",
       "              'intervals_ccnt_13',\n",
       "              'intervals_ccnt_14',\n",
       "              'intervals_ccnt_15',\n",
       "              'num_pkts_in',\n",
       "              'num_pkts_out',\n",
       "              'pld_bin_inf',\n",
       "              'pld_ccnt_0',\n",
       "              'pld_ccnt_1',\n",
       "              'pld_ccnt_2',\n",
       "              'pld_ccnt_3',\n",
       "              'pld_ccnt_4',\n",
       "              'pld_ccnt_5',\n",
       "              'pld_ccnt_6',\n",
       "              'pld_ccnt_7',\n",
       "              'pld_ccnt_8',\n",
       "              'pld_ccnt_9',\n",
       "              'pld_ccnt_10',\n",
       "              'pld_ccnt_11',\n",
       "              'pld_ccnt_12',\n",
       "              'pld_ccnt_13',\n",
       "              'pld_ccnt_14',\n",
       "              'pld_ccnt_15',\n",
       "              'pld_distinct',\n",
       "              'pld_max',\n",
       "              'pld_mean',\n",
       "              'pld_median',\n",
       "              'pr',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_0',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_1',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_2',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_3',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_4',\n",
       "              'rev_hdr_bin_40',\n",
       "              'rev_hdr_ccnt_0',\n",
       "              'rev_hdr_ccnt_1',\n",
       "              'rev_hdr_ccnt_2',\n",
       "              'rev_hdr_ccnt_3',\n",
       "              'rev_hdr_ccnt_4',\n",
       "              'rev_hdr_ccnt_5',\n",
       "              'rev_hdr_ccnt_6',\n",
       "              'rev_hdr_ccnt_7',\n",
       "              'rev_hdr_ccnt_8',\n",
       "              'rev_hdr_ccnt_9',\n",
       "              'rev_hdr_ccnt_10',\n",
       "              'rev_hdr_ccnt_11',\n",
       "              'rev_hdr_distinct',\n",
       "              'rev_intervals_ccnt_0',\n",
       "              'rev_intervals_ccnt_1',\n",
       "              'rev_intervals_ccnt_2',\n",
       "              'rev_intervals_ccnt_3',\n",
       "              'rev_intervals_ccnt_4',\n",
       "              'rev_intervals_ccnt_5',\n",
       "              'rev_intervals_ccnt_6',\n",
       "              'rev_intervals_ccnt_7',\n",
       "              'rev_intervals_ccnt_8',\n",
       "              'rev_intervals_ccnt_9',\n",
       "              'rev_intervals_ccnt_10',\n",
       "              'rev_intervals_ccnt_11',\n",
       "              'rev_intervals_ccnt_12',\n",
       "              'rev_intervals_ccnt_13',\n",
       "              'rev_intervals_ccnt_14',\n",
       "              'rev_intervals_ccnt_15',\n",
       "              'rev_pld_bin_128',\n",
       "              'rev_pld_ccnt_0',\n",
       "              'rev_pld_ccnt_1',\n",
       "              'rev_pld_ccnt_2',\n",
       "              'rev_pld_ccnt_3',\n",
       "              'rev_pld_ccnt_4',\n",
       "              'rev_pld_ccnt_5',\n",
       "              'rev_pld_ccnt_6',\n",
       "              'rev_pld_ccnt_7',\n",
       "              'rev_pld_ccnt_8',\n",
       "              'rev_pld_ccnt_9',\n",
       "              'rev_pld_ccnt_10',\n",
       "              'rev_pld_ccnt_11',\n",
       "              'rev_pld_ccnt_12',\n",
       "              'rev_pld_ccnt_13',\n",
       "              'rev_pld_ccnt_14',\n",
       "              'rev_pld_ccnt_15',\n",
       "              'rev_pld_distinct',\n",
       "              'rev_pld_max',\n",
       "              'rev_pld_mean',\n",
       "              'rev_pld_var',\n",
       "              'src_port',\n",
       "              'time_length',\n",
       "              'y']})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "data_types = defaultdict(list)\n",
    "\n",
    "for entry in data.schema.fields:\n",
    "    data_types[str(entry.dataType)].append(entry.name)\n",
    "    \n",
    "data_types    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# strings_used = [var for var in data_types[\"StringType\"]]\n",
    "# stage_string = [StringIndexer(inputCol= c, outputCol= c +\"_string_encoded\") for c in strings_used]\n",
    "# stage_one_hot = [OneHotEncoder(inputCol= c + \"_string_encoded\", outputCol= c + \"_one_hot\") for c in strings_used]\n",
    "\n",
    "# ppl = Pipeline(stages= stage_string + stage_one_hot)\n",
    "# df = ppl.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/CICIDS2017_fine/X_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='y')\n",
    "y = df.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 441116 entries, 0 to 441115\n",
      "Columns: 121 entries, ack_psh_rst_syn_fin_cnt_0 to time_length\n",
      "dtypes: float64(121)\n",
      "memory usage: 407.2 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
