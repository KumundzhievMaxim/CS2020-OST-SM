{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inquirer in /opt/conda/lib/python3.8/site-packages (2.7.0)\r\n",
      "Requirement already satisfied: blessed==1.17.6 in /opt/conda/lib/python3.8/site-packages (from inquirer) (1.17.6)\r\n",
      "Requirement already satisfied: python-editor==1.0.4 in /opt/conda/lib/python3.8/site-packages (from inquirer) (1.0.4)\r\n",
      "Requirement already satisfied: readchar==2.0.1 in /opt/conda/lib/python3.8/site-packages (from inquirer) (2.0.1)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.8/site-packages (from blessed==1.17.6->inquirer) (0.2.5)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from blessed==1.17.6->inquirer) (1.15.0)\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "!pip install inquirer\n",
    "import inquirer\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorSlicer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Take datasett name from user ( + option ALL)\n",
    "\n",
    "# 2. Which feature selection user want to apply\n",
    "\n",
    "# 3. selected/train.csv, test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SparkFactory:\n",
    "    def __init__(self):\n",
    "        self.base = Path('./datasets')\n",
    "        self.aviable_datasets = [\"CICIDS\", \"NET\", \"All\"]\n",
    "        self.spark = SparkSession.builder.appName(__name__).getOrCreate()\n",
    "    \n",
    "    def validate_available_dataset(self):\n",
    "#         return [name for name in glob.glob(f'{self.source_dataset_folder}/**')]\n",
    "        return [name for name in self.aviable_datasets]\n",
    "\n",
    "    def preprocess(dataframe):\n",
    "        dataframe.fillna(dataframe.mean(), inplace=True)\n",
    "        TARGETS = ['binary_class', 'multi_class']\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "        for target_column in TARGETS: \n",
    "            dataframe[f'{target_column}'] = ordinal_encoder.fit_transform(dataframe[[f'{target_column}']]).astype('int')\n",
    "        \n",
    "        X = dataframe.drop(['binary_class','multi_class'], axis=1)\n",
    "        y = dataframe.drop()\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def read_dataset(self, dataset):\n",
    "        \"\"\"Read source dataset with sql interface\"\"\"\n",
    "        print(f'Reading dataset: {dataset}')\n",
    "        try:\n",
    "            self.spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(dataset) \n",
    "            data = self.spark.read.option(\"header\", \"true\").csv(dataset)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def read_df(self, dataset):\n",
    "        df = self.spark.createDataFrame(pd.read_csv(dataset))\n",
    "        return df\n",
    "    \n",
    "    def _one_hot_endcoder(data):\n",
    "        \n",
    "        Vectorizer = CountVectorizer(inputCol=\"Color_Array\", outputCol=\"Color_OneHotEncoded\", vocabSize=4, minDF=1.0)\n",
    "        return data\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        print(self.base)\n",
    "        # validate and choose dataset to work with\n",
    "        assert input('Hi! Would u like to see available datasets to process? Y/n: ') == 'Y', 'Okay, see u later!'    \n",
    "        print(self.validate_available_dataset())\n",
    "        dataset = input('Good! Choose one of datasets to work with pasting name shown in previous step: ')\n",
    "        \n",
    "        while False:\n",
    "            path = f'' + self.source_dataset_folder + '/TRANSFORMED_{dataset}'\n",
    "            print(path)\n",
    "            assert os.path.isdir(path) == True, 'Wrong path, try one more time!'\n",
    "        print(f'Nice, we gonna to preprocess {dataset}')\n",
    "        data = self.read_dataset(dataset)\n",
    "        # print(data.toPandas().head(10))\n",
    "        \n",
    "        # validate and choose method to use\n",
    "        print('Available methods to apply: [OHE, B, C, D]')\n",
    "        method = input('Now choose one of methods to apply ')\n",
    "        if method == 'OHE':\n",
    "            data = _one_hot_endcoder(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkFactory().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMED_CICIDS_TRAIN = pd.read_csv('datasets/TRANSFORMED_CICIDS/train.csv')\n",
    "\n",
    "_df = SparkFactory.preprocess(TRANSFORMED_CICIDS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(__name__).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[src_port: bigint, pld_distinct: bigint, bytes_out: bigint, hdr_mean: double, num_pkts_out: bigint, pld_mean: double, rev_hdr_distinct: bigint, hdr_bin_40: bigint, pr: bigint, rev_hdr_bin_40: bigint, pld_bin_inf: bigint, rev_pld_mean: double, rev_pld_bin_128: bigint, hdr_distinct: bigint, dns_answer_cnt: double, id: bigint, dns_query_cnt: double, pld_median: bigint, time_length: double, rev_pld_distinct: bigint, num_pkts_in: bigint, rev_pld_var: double, pld_max: bigint, rev_pld_max: bigint, dst_port: bigint, bytes_in: bigint, tls_svr_key_exchange_len: double, tls_svr_cnt: double, tls_cnt: double, tls_key_exchange_len: double, tls_svr_ext_cnt: double, tls_svr_cs_cnt: double, tls_cs_cnt: double, tls_ext_cnt: double, http_method: double, http_code: double, http_content_len: double]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[src_port: bigint, pld_distinct: bigint, bytes_out: bigint, hdr_mean: double, num_pkts_out: bigint, pld_mean: double, rev_hdr_distinct: bigint, hdr_bin_40: bigint, pr: bigint, rev_hdr_bin_40: bigint, pld_bin_inf: bigint, rev_pld_mean: double, rev_pld_bin_128: bigint, hdr_distinct: bigint, dns_answer_cnt: double, id: bigint, dns_query_cnt: double, pld_median: bigint, time_length: double, rev_pld_distinct: bigint, num_pkts_in: bigint, rev_pld_var: double, pld_max: bigint, rev_pld_max: bigint, dst_port: bigint, bytes_in: bigint, tls_svr_key_exchange_len: double, tls_svr_cnt: double, tls_cnt: double, tls_key_exchange_len: double, tls_svr_ext_cnt: double, tls_svr_cs_cnt: double, tls_cs_cnt: double, tls_ext_cnt: double, http_method: double, http_code: double, http_content_len: double, binary_class: bigint, multi_class: bigint]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['multi_class']\n",
    "\n",
    "spark_df_ = spark_df_.select([column for column in spark_df_.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[src_port: bigint, pld_distinct: bigint, bytes_out: bigint, hdr_mean: double, num_pkts_out: bigint, pld_mean: double, rev_hdr_distinct: bigint, hdr_bin_40: bigint, pr: bigint, rev_hdr_bin_40: bigint, pld_bin_inf: bigint, rev_pld_mean: double, rev_pld_bin_128: bigint, hdr_distinct: bigint, dns_answer_cnt: double, id: bigint, dns_query_cnt: double, pld_median: bigint, time_length: double, rev_pld_distinct: bigint, num_pkts_in: bigint, rev_pld_var: double, pld_max: bigint, rev_pld_max: bigint, dst_port: bigint, bytes_in: bigint, tls_svr_key_exchange_len: double, tls_svr_cnt: double, tls_cnt: double, tls_key_exchange_len: double, tls_svr_ext_cnt: double, tls_svr_cs_cnt: double, tls_cs_cnt: double, tls_ext_cnt: double, http_method: double, http_code: double, http_content_len: double, binary_class: bigint]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var = [i[0] for i in spark_df_.dtypes if (((i[1]=='int') | (i[1]=='bigint') | (i[1]=='double')) & (i[0]!='binary_class'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexes = StringIndexer(inputCol = 'binary_class', outputCol = 'label', handleInvalid = 'keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = num_var, outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages = [assembler, label_indexes, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = pipe.fit(spark_df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = mod.transform(spark_df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(37, {0: 0.0027, 1: 0.0101, 2: 0.1089, 3: 0.0952, 4: 0.0041, 5: 0.1645, 6: 0.0011, 7: 0.0096, 8: 0.0004, 9: 0.0086, 10: 0.0003, 11: 0.0898, 12: 0.018, 13: 0.0106, 15: 0.0, 17: 0.0116, 18: 0.028, 19: 0.0002, 20: 0.0055, 21: 0.065, 22: 0.093, 23: 0.0791, 24: 0.0583, 25: 0.0763, 27: 0.008, 28: 0.0025, 29: 0.0055, 30: 0.0014, 32: 0.0021, 33: 0.0031, 34: 0.0, 35: 0.0059, 36: 0.0305})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist = ExtractFeatureImp(mod.stages[-1].featureImportances, df2, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 3, 22, 11, 23, 25, 21, 24, 36]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varidx = [x for x in varlist['idx'][0:10]]\n",
    "varidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer = VectorSlicer(inputCol=\"features\", outputCol=\"features2\", indices=varidx)\n",
    "df3 = slicer.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop('rawPrediction', 'probability', 'prediction')\n",
    "rf2 = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features2\", seed = 8464,\n",
    "                            numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "mod2 = rf2.fit(df3)\n",
    "df4 = mod2.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[src_port: bigint, pld_distinct: bigint, bytes_out: bigint, hdr_mean: double, num_pkts_out: bigint, pld_mean: double, rev_hdr_distinct: bigint, hdr_bin_40: bigint, pr: bigint, rev_hdr_bin_40: bigint, pld_bin_inf: bigint, rev_pld_mean: double, rev_pld_bin_128: bigint, hdr_distinct: bigint, dns_answer_cnt: double, id: bigint, dns_query_cnt: double, pld_median: bigint, time_length: double, rev_pld_distinct: bigint, num_pkts_in: bigint, rev_pld_var: double, pld_max: bigint, rev_pld_max: bigint, dst_port: bigint, bytes_in: bigint, tls_svr_key_exchange_len: double, tls_svr_cnt: double, tls_cnt: double, tls_key_exchange_len: double, tls_svr_ext_cnt: double, tls_svr_cs_cnt: double, tls_cs_cnt: double, tls_ext_cnt: double, http_method: double, http_code: double, http_content_len: double, binary_class: bigint, features: vector, label: double, features2: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_var = [i[0] for i in df.dtypes if (i[1]=='string') & (i[0]!='y')]\n",
    "num_var = [i[0] for i in df.dtypes if ((i[1]=='int') | (i[1]=='double')) & (i[0]!='y')]\n",
    "\n",
    "string_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep') for c in encoding_var]\n",
    "onehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c]) for c in encoding_var]\n",
    "label_indexes = StringIndexer(inputCol = 'y', outputCol = 'label', handleInvalid = 'keep')\n",
    "assembler = VectorAssembler(inputCols = num_var + ['OHE_' + c for c in encoding_var], outputCol = \"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,\n",
    "                            numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "\n",
    "pipe = Pipeline(stages = string_indexes + onehot_indexes + [assembler, label_indexes, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_port</th>\n",
       "      <th>pld_distinct</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>hdr_mean</th>\n",
       "      <th>num_pkts_out</th>\n",
       "      <th>pld_mean</th>\n",
       "      <th>rev_hdr_distinct</th>\n",
       "      <th>hdr_bin_40</th>\n",
       "      <th>pr</th>\n",
       "      <th>rev_hdr_bin_40</th>\n",
       "      <th>...</th>\n",
       "      <th>tls_key_exchange_len</th>\n",
       "      <th>tls_svr_ext_cnt</th>\n",
       "      <th>tls_svr_cs_cnt</th>\n",
       "      <th>tls_cs_cnt</th>\n",
       "      <th>tls_ext_cnt</th>\n",
       "      <th>http_method</th>\n",
       "      <th>http_code</th>\n",
       "      <th>http_content_len</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>multi_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56565</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.037562</td>\n",
       "      <td>2.415821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.295687</td>\n",
       "      <td>7.264552</td>\n",
       "      <td>1.195444</td>\n",
       "      <td>206.271259</td>\n",
       "      <td>64397.744691</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52995</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>37.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.037562</td>\n",
       "      <td>2.415821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.295687</td>\n",
       "      <td>7.264552</td>\n",
       "      <td>1.195444</td>\n",
       "      <td>206.271259</td>\n",
       "      <td>64397.744691</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40805</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>35.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.037562</td>\n",
       "      <td>2.415821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.295687</td>\n",
       "      <td>7.264552</td>\n",
       "      <td>1.195444</td>\n",
       "      <td>206.271259</td>\n",
       "      <td>64397.744691</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31833</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>36.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.037562</td>\n",
       "      <td>2.415821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.295687</td>\n",
       "      <td>7.264552</td>\n",
       "      <td>1.195444</td>\n",
       "      <td>206.271259</td>\n",
       "      <td>64397.744691</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64062</td>\n",
       "      <td>7</td>\n",
       "      <td>3272</td>\n",
       "      <td>32.8</td>\n",
       "      <td>15</td>\n",
       "      <td>218.13</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.195444</td>\n",
       "      <td>206.271259</td>\n",
       "      <td>64397.744691</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   src_port  pld_distinct  bytes_out  hdr_mean  num_pkts_out  pld_mean  \\\n",
       "0     56565             1         58       8.0             2     29.00   \n",
       "1     52995             1        148       8.0             4     37.00   \n",
       "2     40805             1         70       8.0             2     35.00   \n",
       "3     31833             2        146       8.0             4     36.50   \n",
       "4     64062             7       3272      32.8            15    218.13   \n",
       "\n",
       "   rev_hdr_distinct  hdr_bin_40  pr  rev_hdr_bin_40  ...  \\\n",
       "0                 1           0  17               0  ...   \n",
       "1                 1           0  17               0  ...   \n",
       "2                 1           0  17               0  ...   \n",
       "3                 1           0  17               0  ...   \n",
       "4                 3          14   6              12  ...   \n",
       "\n",
       "   tls_key_exchange_len  tls_svr_ext_cnt  tls_svr_cs_cnt  tls_cs_cnt  \\\n",
       "0             71.037562         2.415821             1.0   17.295687   \n",
       "1             71.037562         2.415821             1.0   17.295687   \n",
       "2             71.037562         2.415821             1.0   17.295687   \n",
       "3             71.037562         2.415821             1.0   17.295687   \n",
       "4            258.000000         1.000000             1.0   19.000000   \n",
       "\n",
       "   tls_ext_cnt  http_method   http_code  http_content_len  binary_class  \\\n",
       "0     7.264552     1.195444  206.271259      64397.744691             0   \n",
       "1     7.264552     1.195444  206.271259      64397.744691             0   \n",
       "2     7.264552     1.195444  206.271259      64397.744691             0   \n",
       "3     7.264552     1.195444  206.271259      64397.744691             0   \n",
       "4     1.000000     1.195444  206.271259      64397.744691             0   \n",
       "\n",
       "   multi_class  \n",
       "0            2  \n",
       "1            2  \n",
       "2            2  \n",
       "3            2  \n",
       "4            2  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled Nan values for dataset\n",
      "Sanity check: Nan values before: src_port                    0\n",
      "pld_distinct                0\n",
      "bytes_out                   0\n",
      "hdr_mean                    0\n",
      "num_pkts_out                0\n",
      "pld_mean                    0\n",
      "rev_hdr_distinct            0\n",
      "hdr_bin_40                  0\n",
      "pr                          0\n",
      "rev_hdr_bin_40              0\n",
      "pld_bin_inf                 0\n",
      "rev_pld_mean                0\n",
      "rev_pld_bin_128             0\n",
      "hdr_distinct                0\n",
      "dns_answer_cnt              0\n",
      "id                          0\n",
      "dns_query_cnt               0\n",
      "pld_median                  0\n",
      "time_length                 0\n",
      "rev_pld_distinct            0\n",
      "num_pkts_in                 0\n",
      "rev_pld_var                 0\n",
      "pld_max                     0\n",
      "rev_pld_max                 0\n",
      "dst_port                    0\n",
      "bytes_in                    0\n",
      "tls_svr_key_exchange_len    0\n",
      "tls_svr_cnt                 0\n",
      "tls_cnt                     0\n",
      "tls_key_exchange_len        0\n",
      "tls_svr_ext_cnt             0\n",
      "tls_svr_cs_cnt              0\n",
      "tls_cs_cnt                  0\n",
      "tls_ext_cnt                 0\n",
      "http_method                 0\n",
      "http_code                   0\n",
      "http_content_len            0\n",
      "binary_class                0\n",
      "multi_class                 0\n",
      "dtype: int64, after: src_port                    0\n",
      "pld_distinct                0\n",
      "bytes_out                   0\n",
      "hdr_mean                    0\n",
      "num_pkts_out                0\n",
      "pld_mean                    0\n",
      "rev_hdr_distinct            0\n",
      "hdr_bin_40                  0\n",
      "pr                          0\n",
      "rev_hdr_bin_40              0\n",
      "pld_bin_inf                 0\n",
      "rev_pld_mean                0\n",
      "rev_pld_bin_128             0\n",
      "hdr_distinct                0\n",
      "dns_answer_cnt              0\n",
      "id                          0\n",
      "dns_query_cnt               0\n",
      "pld_median                  0\n",
      "time_length                 0\n",
      "rev_pld_distinct            0\n",
      "num_pkts_in                 0\n",
      "rev_pld_var                 0\n",
      "pld_max                     0\n",
      "rev_pld_max                 0\n",
      "dst_port                    0\n",
      "bytes_in                    0\n",
      "tls_svr_key_exchange_len    0\n",
      "tls_svr_cnt                 0\n",
      "tls_cnt                     0\n",
      "tls_key_exchange_len        0\n",
      "tls_svr_ext_cnt             0\n",
      "tls_svr_cs_cnt              0\n",
      "tls_cs_cnt                  0\n",
      "tls_ext_cnt                 0\n",
      "http_method                 0\n",
      "http_code                   0\n",
      "http_content_len            0\n",
      "binary_class                0\n",
      "multi_class                 0\n",
      "dtype: int64\n",
      "Encoded column binary_class with class number: [0 1]\n",
      "Encoded column multi_class with class number: [2 0 1 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMED_CICIDS_TRAIN = pd.read_csv('datasets/TRANSFORMED_CICIDS/train.csv')\n",
    "#TRANSFORMED_CICIDS_TEST = pd.read_csv('datasets/TRANSFORMED_CICIDS/train.csv')\n",
    "\n",
    "TRANSFORMED_NET_TRAIN = pd.read_csv('datasets/TRANSFORMED_NET/train.csv')\n",
    "#TRANSFORMED_NET_TEST = pd.read_csv('datasets/TRANSFORMED_CICIDS/train.csv')\n",
    "\n",
    "TARGETS = ['binary_class', 'multi_class']\n",
    "\n",
    "def preprocess(dataframe: pd.DataFrame):\n",
    "    dataframe.fillna(dataframe.mean(), inplace=True)\n",
    "    print(f'Filled Nan values for dataset')\n",
    "    print(f'Sanity check: Nan values before: {dataframe.isna().sum()}, after: {dataframe.isna().sum()}')\n",
    "    \n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    for target_column in TARGETS: \n",
    "        dataframe[f'{target_column}'] = ordinal_encoder.fit_transform(dataframe[[f'{target_column}']]).astype('int')\n",
    "        print(f'Encoded column {target_column} with class number: {dataframe[f\"{target_column}\"].unique()}')\n",
    "    return dataframe\n",
    "\n",
    "df = preprocess(TRANSFORMED_CICIDS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset: datasets/CICIDS2017/train/fine/train.csv\n"
     ]
    }
   ],
   "source": [
    "# SparkFactory().run()\n",
    "data = SparkFactory().read_dataset('datasets/CICIDS2017/train/fine/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "categorical_columns = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "numerical_columns = [f.name for f in data.schema.fields]\n",
    "print(len(categorical_columns))\n",
    "print(len(numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'StringType': ['_c0',\n",
       "              'ack_psh_rst_syn_fin_cnt_0',\n",
       "              'ack_psh_rst_syn_fin_cnt_1',\n",
       "              'ack_psh_rst_syn_fin_cnt_2',\n",
       "              'ack_psh_rst_syn_fin_cnt_3',\n",
       "              'ack_psh_rst_syn_fin_cnt_4',\n",
       "              'bytes_in',\n",
       "              'bytes_out',\n",
       "              'dst_port',\n",
       "              'hdr_bin_40',\n",
       "              'hdr_ccnt_0',\n",
       "              'hdr_ccnt_1',\n",
       "              'hdr_ccnt_2',\n",
       "              'hdr_ccnt_3',\n",
       "              'hdr_ccnt_4',\n",
       "              'hdr_ccnt_5',\n",
       "              'hdr_ccnt_6',\n",
       "              'hdr_ccnt_7',\n",
       "              'hdr_ccnt_8',\n",
       "              'hdr_ccnt_9',\n",
       "              'hdr_ccnt_10',\n",
       "              'hdr_ccnt_11',\n",
       "              'hdr_distinct',\n",
       "              'hdr_mean',\n",
       "              'intervals_ccnt_0',\n",
       "              'intervals_ccnt_1',\n",
       "              'intervals_ccnt_2',\n",
       "              'intervals_ccnt_3',\n",
       "              'intervals_ccnt_4',\n",
       "              'intervals_ccnt_5',\n",
       "              'intervals_ccnt_6',\n",
       "              'intervals_ccnt_7',\n",
       "              'intervals_ccnt_8',\n",
       "              'intervals_ccnt_9',\n",
       "              'intervals_ccnt_10',\n",
       "              'intervals_ccnt_11',\n",
       "              'intervals_ccnt_12',\n",
       "              'intervals_ccnt_13',\n",
       "              'intervals_ccnt_14',\n",
       "              'intervals_ccnt_15',\n",
       "              'num_pkts_in',\n",
       "              'num_pkts_out',\n",
       "              'pld_bin_inf',\n",
       "              'pld_ccnt_0',\n",
       "              'pld_ccnt_1',\n",
       "              'pld_ccnt_2',\n",
       "              'pld_ccnt_3',\n",
       "              'pld_ccnt_4',\n",
       "              'pld_ccnt_5',\n",
       "              'pld_ccnt_6',\n",
       "              'pld_ccnt_7',\n",
       "              'pld_ccnt_8',\n",
       "              'pld_ccnt_9',\n",
       "              'pld_ccnt_10',\n",
       "              'pld_ccnt_11',\n",
       "              'pld_ccnt_12',\n",
       "              'pld_ccnt_13',\n",
       "              'pld_ccnt_14',\n",
       "              'pld_ccnt_15',\n",
       "              'pld_distinct',\n",
       "              'pld_max',\n",
       "              'pld_mean',\n",
       "              'pld_median',\n",
       "              'pr',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_0',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_1',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_2',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_3',\n",
       "              'rev_ack_psh_rst_syn_fin_cnt_4',\n",
       "              'rev_hdr_bin_40',\n",
       "              'rev_hdr_ccnt_0',\n",
       "              'rev_hdr_ccnt_1',\n",
       "              'rev_hdr_ccnt_2',\n",
       "              'rev_hdr_ccnt_3',\n",
       "              'rev_hdr_ccnt_4',\n",
       "              'rev_hdr_ccnt_5',\n",
       "              'rev_hdr_ccnt_6',\n",
       "              'rev_hdr_ccnt_7',\n",
       "              'rev_hdr_ccnt_8',\n",
       "              'rev_hdr_ccnt_9',\n",
       "              'rev_hdr_ccnt_10',\n",
       "              'rev_hdr_ccnt_11',\n",
       "              'rev_hdr_distinct',\n",
       "              'rev_intervals_ccnt_0',\n",
       "              'rev_intervals_ccnt_1',\n",
       "              'rev_intervals_ccnt_2',\n",
       "              'rev_intervals_ccnt_3',\n",
       "              'rev_intervals_ccnt_4',\n",
       "              'rev_intervals_ccnt_5',\n",
       "              'rev_intervals_ccnt_6',\n",
       "              'rev_intervals_ccnt_7',\n",
       "              'rev_intervals_ccnt_8',\n",
       "              'rev_intervals_ccnt_9',\n",
       "              'rev_intervals_ccnt_10',\n",
       "              'rev_intervals_ccnt_11',\n",
       "              'rev_intervals_ccnt_12',\n",
       "              'rev_intervals_ccnt_13',\n",
       "              'rev_intervals_ccnt_14',\n",
       "              'rev_intervals_ccnt_15',\n",
       "              'rev_pld_bin_128',\n",
       "              'rev_pld_ccnt_0',\n",
       "              'rev_pld_ccnt_1',\n",
       "              'rev_pld_ccnt_2',\n",
       "              'rev_pld_ccnt_3',\n",
       "              'rev_pld_ccnt_4',\n",
       "              'rev_pld_ccnt_5',\n",
       "              'rev_pld_ccnt_6',\n",
       "              'rev_pld_ccnt_7',\n",
       "              'rev_pld_ccnt_8',\n",
       "              'rev_pld_ccnt_9',\n",
       "              'rev_pld_ccnt_10',\n",
       "              'rev_pld_ccnt_11',\n",
       "              'rev_pld_ccnt_12',\n",
       "              'rev_pld_ccnt_13',\n",
       "              'rev_pld_ccnt_14',\n",
       "              'rev_pld_ccnt_15',\n",
       "              'rev_pld_distinct',\n",
       "              'rev_pld_max',\n",
       "              'rev_pld_mean',\n",
       "              'rev_pld_var',\n",
       "              'src_port',\n",
       "              'time_length',\n",
       "              'y']})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "data_types = defaultdict(list)\n",
    "\n",
    "for entry in data.schema.fields:\n",
    "    data_types[str(entry.dataType)].append(entry.name)\n",
    "    \n",
    "data_types    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# strings_used = [var for var in data_types[\"StringType\"]]\n",
    "# stage_string = [StringIndexer(inputCol= c, outputCol= c +\"_string_encoded\") for c in strings_used]\n",
    "# stage_one_hot = [OneHotEncoder(inputCol= c + \"_string_encoded\", outputCol= c + \"_one_hot\") for c in strings_used]\n",
    "\n",
    "# ppl = Pipeline(stages= stage_string + stage_one_hot)\n",
    "# df = ppl.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='y')\n",
    "y = df.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 441116 entries, 0 to 441115\n",
      "Columns: 121 entries, ack_psh_rst_syn_fin_cnt_0 to time_length\n",
      "dtypes: float64(121)\n",
      "memory usage: 407.2 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}