{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inquirer in /opt/conda/lib/python3.8/site-packages (2.7.0)\r\n",
      "Requirement already satisfied: readchar==2.0.1 in /opt/conda/lib/python3.8/site-packages (from inquirer) (2.0.1)\r\n",
      "Requirement already satisfied: blessed==1.17.6 in /opt/conda/lib/python3.8/site-packages (from inquirer) (1.17.6)\r\n",
      "Requirement already satisfied: python-editor==1.0.4 in /opt/conda/lib/python3.8/site-packages (from inquirer) (1.0.4)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.8/site-packages (from blessed==1.17.6->inquirer) (0.2.5)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from blessed==1.17.6->inquirer) (1.15.0)\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "!pip install inquirer\n",
    "import inquirer\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorSlicer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SparkFactory:\n",
    "    def __init__(self):\n",
    "        self.base = Path('./datasets')\n",
    "        self.aviable_datasets = [\"CICIDS\", \"NET\", \"All\"]\n",
    "        self.spark = SparkSession.builder.appName(__name__).getOrCreate()\n",
    "    \n",
    "    def validate_available_dataset(self):\n",
    "#         return [name for name in glob.glob(f'{self.source_dataset_folder}/**')]\n",
    "        return [name for name in self.aviable_datasets]\n",
    "\n",
    "    def preprocess(dataframe):\n",
    "        dataframe.fillna(dataframe.mean(), inplace=True)\n",
    "        TARGETS = ['binary_class', 'multi_class']\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "        for target_column in TARGETS: \n",
    "            dataframe[f'{target_column}'] = ordinal_encoder.fit_transform(dataframe[[f'{target_column}']]).astype('int')        \n",
    "        return dataframe\n",
    "\n",
    "    def read_dataset(self, dataset):\n",
    "        \"\"\"Read source dataset with sql interface\"\"\"\n",
    "        print(f'Reading dataset: {dataset}')\n",
    "        try:\n",
    "            self.spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(dataset) \n",
    "            data = self.spark.read.option(\"header\", \"true\").csv(dataset)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def read_df(self, dataset):\n",
    "        df = self.spark.createDataFrame(pd.read_csv(dataset))\n",
    "        return df\n",
    "    \n",
    "    def _one_hot_endcoder(data):\n",
    "        \n",
    "        Vectorizer = CountVectorizer(inputCol=\"Color_Array\", outputCol=\"Color_OneHotEncoded\", vocabSize=4, minDF=1.0)\n",
    "        return data\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        print(self.base)\n",
    "        # validate and choose dataset to work with\n",
    "        assert input('Hi! Would u like to see available datasets to process? Y/n: ') == 'Y', 'Okay, see u later!'    \n",
    "        print(self.validate_available_dataset())\n",
    "        dataset = input('Good! Choose one of datasets to work with pasting name shown in previous step: ')\n",
    "        \n",
    "        while False:\n",
    "            path = f'' + self.source_dataset_folder + '/TRANSFORMED_{dataset}'\n",
    "            print(path)\n",
    "            assert os.path.isdir(path) == True, 'Wrong path, try one more time!'\n",
    "        print(f'Nice, we gonna to preprocess {dataset}')\n",
    "        data = self.read_dataset(dataset)\n",
    "        # print(data.toPandas().head(10))\n",
    "        \n",
    "        # validate and choose method to use\n",
    "        print('Available methods to apply: [OHE, B, C, D]')\n",
    "        method = input('Now choose one of methods to apply ')\n",
    "        if method == 'OHE':\n",
    "            data = _one_hot_endcoder(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkFactory().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pd():\n",
    "    TRANSFORMED_CICIDS_TRAIN = pd.read_csv('datasets/TRANSFORMED_CICIDS/train.csv')\n",
    "    TRANSFORMED_CICIDS_TEST = pd.read_csv('datasets/TRANSFORMED_CICIDS/test.csv')\n",
    "\n",
    "    TRANSFORMED_NET_TRAIN = pd.read_csv('datasets/TRANSFORMED_NET/train.csv')\n",
    "    TRANSFORMED_NET_TEST = pd.read_csv('datasets/TRANSFORMED_NET/test.csv')\n",
    "    \n",
    "    return TRANSFORMED_CICIDS_TRAIN, TRANSFORMED_CICIDS_TEST, TRANSFORMED_NET_TRAIN, TRANSFORMED_NET_TEST\n",
    "\n",
    "TRANSFORMED_CICIDS_TRAIN, TRANSFORMED_CICIDS_TEST, TRANSFORMED_NET_TRAIN, TRANSFORMED_NET_TEST = read_pd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace NaN to mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataframe, is_test = False):\n",
    "    dataframe.fillna(dataframe.mean(), inplace=True)\n",
    "    \n",
    "    if is_test == True:\n",
    "        TARGETS = ['binary_class', 'multi_class']\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "        for target_column in TARGETS: \n",
    "            dataframe[f'{target_column}'] = ordinal_encoder.fit_transform(dataframe[[f'{target_column}']]).astype('int')        \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "pd_cicids_train = preprocess(TRANSFORMED_CICIDS_TRAIN, True)\n",
    "pd_cicids_test = preprocess(TRANSFORMED_CICIDS_TEST)\n",
    "\n",
    "pd_net_train = preprocess(TRANSFORMED_NET_TRAIN, True)\n",
    "pd_net_test = preprocess(TRANSFORMED_NET_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns_cicids = pd_cicids_train[['binary_class', 'multi_class']]\n",
    "target_columns_net = pd_net_train[['binary_class', 'multi_class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(__name__).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read DataFrames to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CICIDS\n",
    "pd_cicids_train = spark.createDataFrame(pd_cicids_train)\n",
    "# pd_cicids_test = spark.createDataFrame(pd_cicids_test)\n",
    "\n",
    "#NET\n",
    "pd_net_train = spark.createDataFrame(pd_net_train)\n",
    "# pd_net_test = spark.createDataFrame(pd_net_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop one target to fit Feature Importance model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['multi_class']\n",
    "\n",
    "pd_cicids_train = pd_cicids_train.select([column for column in pd_cicids_train.columns if column not in drop_list])\n",
    "pd_net_train = pd_net_train.select([column for column in pd_net_train.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Feature Importance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_preparation(df):\n",
    "    \n",
    "    num_var = [i[0] for i in df.dtypes if (((i[1]=='int') | (i[1]=='bigint') | (i[1]=='double')) & (i[0]!='binary_class'))]\n",
    "\n",
    "    label_indexes = StringIndexer(inputCol = 'binary_class', outputCol = 'label', handleInvalid = 'keep')\n",
    "    assembler = VectorAssembler(inputCols = num_var, outputCol = \"features\")\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "\n",
    "    stages = [assembler, label_indexes, rf]\n",
    "    \n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pipelines\n",
    "\n",
    "cicids_pipeline =  Pipeline(stages = pipeline_preparation(pd_cicids_train))\n",
    "net_pipeline    =  Pipeline(stages = pipeline_preparation(pd_net_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipelines\n",
    "\n",
    "cicids_model = cicids_pipeline.fit(pd_cicids_train)\n",
    "net_model    = net_pipeline.fit(pd_net_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform DataFrames\n",
    "\n",
    "pd_cicids_train_ = cicids_model.transform(pd_cicids_train)\n",
    "pd_net_train_ = net_model.transform(pd_net_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check feature importance vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(37, {0: 0.0027, 1: 0.0101, 2: 0.1089, 3: 0.0952, 4: 0.0041, 5: 0.1645, 6: 0.0011, 7: 0.0096, 8: 0.0004, 9: 0.0086, 10: 0.0003, 11: 0.0898, 12: 0.018, 13: 0.0106, 15: 0.0, 17: 0.0116, 18: 0.028, 19: 0.0002, 20: 0.0055, 21: 0.065, 22: 0.093, 23: 0.0791, 24: 0.0583, 25: 0.0763, 27: 0.008, 28: 0.0025, 29: 0.0055, 30: 0.0014, 32: 0.0021, 33: 0.0031, 34: 0.0, 35: 0.0059, 36: 0.0305})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cicids_model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(37, {0: 0.0078, 2: 0.0501, 3: 0.1213, 4: 0.0032, 5: 0.0003, 6: 0.018, 7: 0.1068, 8: 0.0208, 9: 0.0246, 10: 0.1439, 11: 0.1664, 12: 0.0035, 13: 0.0011, 14: 0.0071, 15: 0.0041, 16: 0.0341, 17: 0.0437, 18: 0.0119, 19: 0.003, 20: 0.0201, 21: 0.001, 22: 0.0401, 23: 0.0, 28: 0.004, 29: 0.0009, 31: 0.0313, 32: 0.0035, 33: 0.0194, 35: 0.0191, 36: 0.0889})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist_cicids = ExtractFeatureImp(cicids_model.stages[-1].featureImportances, pd_cicids_train_, \"features\")\n",
    "varlist_net = ExtractFeatureImp(net_model.stages[-1].featureImportances, pd_net_train_, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "varidx_cicids = [x for x in varlist_cicids['idx'][0:15]]\n",
    "varidxt_net= [x for x in varlist_net['idx'][0:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer_cicids = VectorSlicer(inputCol=\"features\", outputCol=\"features2\", indices=varidx_cicids)\n",
    "selected_cicids_train_ = slicer_cicids.transform(pd_cicids_train_)\n",
    "\n",
    "slicer_net = VectorSlicer(inputCol=\"features\", outputCol=\"features2\", indices=varidxt_net)\n",
    "selected_net_train_ = slicer_net.transform(pd_net_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_features_cicids = list(ExtractFeatureImp(cicids_model.stages[-1].featureImportances, pd_cicids_train_, \"features\").head(20).name)\n",
    "import_features_net = list(ExtractFeatureImp(net_model.stages[-1].featureImportances, pd_net_train_, \"features\").head(20).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_selected_cicids_train = selected_cicids_train_.toPandas()\n",
    "pd_selected_net_train = pd_net_train_.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_selected_cicids_train = pd_selected_cicids_train[import_features_cicids]\n",
    "pd_selected_net_train = pd_selected_net_train[import_features_net]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(dataset, dataset_name, dataset_type):\n",
    "    DESTINATION = Path('datasets', f'SELECTED_{dataset_name}')\n",
    "    DESTINATION.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    FILE = DESTINATION / f'{dataset_type}.csv'\n",
    "    \n",
    "    dataset.to_csv(FILE, index=False)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCAT LABELS AND DATASETS\n",
    "train_cicids = pd_selected_cicids_train + target_columns_cicids\n",
    "test_cicids = pd_cicids_test[import_features_cicids]\n",
    "\n",
    "train_net = pd_selected_net_train + target_columns_net\n",
    "test_net = pd_net_test[import_features_net]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(dataset=train_cicids, dataset_name='CICIDS', dataset_type='train')\n",
    "save_csv(dataset=test_cicids, dataset_name='CICIDS', dataset_type='test')\n",
    "\n",
    "save_csv(dataset=train_net, dataset_name='NET', dataset_type='train')\n",
    "save_csv(dataset=test_net, dataset_name='NET', dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
