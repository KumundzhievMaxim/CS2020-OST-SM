{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inquirer in /opt/conda/lib/python3.8/site-packages (2.7.0)\r\n",
      "Requirement already satisfied: readchar==2.0.1 in /opt/conda/lib/python3.8/site-packages (from inquirer) (2.0.1)\r\n",
      "Requirement already satisfied: blessed==1.17.6 in /opt/conda/lib/python3.8/site-packages (from inquirer) (1.17.6)\r\n",
      "Requirement already satisfied: python-editor==1.0.4 in /opt/conda/lib/python3.8/site-packages (from inquirer) (1.0.4)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from blessed==1.17.6->inquirer) (1.15.0)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.8/site-packages (from blessed==1.17.6->inquirer) (0.2.5)\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "!pip install inquirer\n",
    "import inquirer\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorSlicer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SparkFactory:\n",
    "    def __init__(self):\n",
    "        self.base = Path('./datasets')\n",
    "        self.aviable_datasets = [\"CICIDS\", \"NET\", \"All\"]\n",
    "        self.aviable_methods = [\"VectorSlicer\", \"ChiSqSelector\"]\n",
    "        self.spark = SparkSession.builder.appName(__name__).getOrCreate()\n",
    "    \n",
    "    def validate_available_dataset(self):\n",
    "        return [name for name in self.aviable_datasets]\n",
    "\n",
    "    def validate_available_methods(self):\n",
    "        return [name for name in self.aviable_methods]\n",
    "    \n",
    "    def validate_name_method(self, name):\n",
    "        if name not in self.aviable_methods:\n",
    "            return false\n",
    "        return true\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        print(self.base)\n",
    "        assert input('Hi! Would u like to see available datasets to process? Y/n: ') == 'Y', 'Okay, see u later!'    \n",
    "        print(self.validate_available_dataset())\n",
    "        \n",
    "        dataset = input('Good! Choose one of datasets to work with pasting name shown in previous step: ')\n",
    "        \n",
    "        print(\"Nice, now we will preprocess the data\")\n",
    "        \n",
    "        return self.process(dataset)\n",
    "        \n",
    "    \n",
    "    def process(self, dataset):\n",
    "             \n",
    "        TRANSFORMED_CICIDS_TRAIN, TRANSFORMED_CICIDS_TEST, TRANSFORMED_NET_TRAIN, TRANSFORMED_NET_TEST = self.read_df()\n",
    "        \n",
    "        pd_cicids_train = self.preprocess(TRANSFORMED_CICIDS_TRAIN, True)\n",
    "        pd_cicids_test = self.preprocess(TRANSFORMED_CICIDS_TEST)\n",
    "\n",
    "        pd_net_train = self.preprocess(TRANSFORMED_NET_TRAIN, True)\n",
    "        pd_net_test = self.preprocess(TRANSFORMED_NET_TEST)\n",
    "        \n",
    "        target_columns_cicids = pd_cicids_train[['binary_class', 'multi_class']]\n",
    "        target_columns_net = pd_net_train[['binary_class', 'multi_class']]\n",
    "       \n",
    "        pd_cicids_train = self.spark.createDataFrame(pd_cicids_train)\n",
    "        pd_net_train = self.spark.createDataFrame(pd_net_train)\n",
    "        \n",
    "        drop_list = ['multi_class']\n",
    "        pd_cicids_train = pd_cicids_train.select([column for column in pd_cicids_train.columns if column not in drop_list])\n",
    "        pd_net_train = pd_net_train.select([column for column in pd_net_train.columns if column not in drop_list])\n",
    "        \n",
    "        print(\"The data is prepared, start doing Feature Selection\")\n",
    "        \n",
    "        cicids_pipeline =  Pipeline(stages = self.pipeline_preparation(pd_cicids_train))\n",
    "        net_pipeline    =  Pipeline(stages = self.pipeline_preparation(pd_net_train))\n",
    "        \n",
    "        cicids_model = cicids_pipeline.fit(pd_cicids_train)\n",
    "        net_model    = net_pipeline.fit(pd_net_train)\n",
    "\n",
    "        pd_cicids_train_ = cicids_model.transform(pd_cicids_train)\n",
    "        pd_net_train_ = net_model.transform(pd_net_train)\n",
    "        \n",
    "        varlist_cicids = self.ExtractFeatureImp(cicids_model.stages[-1].featureImportances, pd_cicids_train_, \"features\")\n",
    "        varlist_net = self.ExtractFeatureImp(net_model.stages[-1].featureImportances, pd_net_train_, \"features\")\n",
    "        \n",
    "        varidx_cicids = [x for x in varlist_cicids['idx'][0:20]]\n",
    "        varidxt_net= [x for x in varlist_net['idx'][0:20]]\n",
    "        \n",
    "        slicer_cicids = VectorSlicer(inputCol=\"features\", outputCol=\"features2\", indices=varidx_cicids)\n",
    "        selected_cicids_train_ = slicer_cicids.transform(pd_cicids_train_)\n",
    "\n",
    "        slicer_net = VectorSlicer(inputCol=\"features\", outputCol=\"features2\", indices=varidxt_net)\n",
    "        selected_net_train_ = slicer_net.transform(pd_net_train_)\n",
    "        \n",
    "        import_features_cicids = list(self.ExtractFeatureImp(cicids_model.stages[-1].featureImportances, pd_cicids_train_, \"features\").head(20).name)\n",
    "        import_features_net = list(self.ExtractFeatureImp(net_model.stages[-1].featureImportances, pd_net_train_, \"features\").head(20).name)\n",
    "        \n",
    "        pd_selected_cicids_train = selected_cicids_train_.toPandas()\n",
    "        pd_selected_net_train = pd_net_train_.toPandas()\n",
    "        \n",
    "        print(\"We know important features, transform the data\")\n",
    "        \n",
    "        pd_selected_cicids_train = pd_selected_cicids_train[import_features_cicids]\n",
    "        pd_selected_net_train = pd_selected_net_train[import_features_net]\n",
    "        \n",
    "        train_cicids = pd.concat([pd_selected_cicids_train, target_columns_cicids], axis = 1)\n",
    "        test_cicids = pd_cicids_test[import_features_cicids]\n",
    "\n",
    "        train_net = pd.concat([pd_selected_net_train, target_columns_net], axis = 1)\n",
    "        test_net = pd_net_test[import_features_net]\n",
    "        \n",
    "        print(\"Saving selected data\")\n",
    "        \n",
    "        if dataset == 'CICIDS':\n",
    "            self.save_csv(dataset=train_cicids, dataset_name='CICIDS', dataset_type='train')\n",
    "            self.save_csv(dataset=test_cicids, dataset_name='CICIDS', dataset_type='test')\n",
    "            \n",
    "        if dataset == 'NET':\n",
    "            self.save_csv(dataset=train_net, dataset_name='NET', dataset_type='train')\n",
    "            self.save_csv(dataset=test_net, dataset_name='NET', dataset_type='test')\n",
    "            \n",
    "        if dataset == 'All':\n",
    "            self.save_csv(dataset=train_cicids, dataset_name='CICIDS', dataset_type='train')\n",
    "            self.save_csv(dataset=test_cicids, dataset_name='CICIDS', dataset_type='test')\n",
    "\n",
    "            self.save_csv(dataset=train_net, dataset_name='NET', dataset_type='train')\n",
    "            self.save_csv(dataset=test_net, dataset_name='NET', dataset_type='test')\n",
    "        \n",
    "        print(\"You can check your new data!\")\n",
    "        \n",
    "        assert True, \"\"\n",
    "\n",
    "    def read_df(self):\n",
    "\n",
    "        TRANSFORMED_CICIDS_TRAIN = pd.read_csv('datasets/TRANSFORMED_CICIDS/train.csv')\n",
    "        TRANSFORMED_CICIDS_TEST = pd.read_csv('datasets/TRANSFORMED_CICIDS/test.csv')\n",
    "\n",
    "        TRANSFORMED_NET_TRAIN = pd.read_csv('datasets/TRANSFORMED_NET/train.csv')\n",
    "        TRANSFORMED_NET_TEST = pd.read_csv('datasets/TRANSFORMED_NET/test.csv')\n",
    "    \n",
    "        return TRANSFORMED_CICIDS_TRAIN, TRANSFORMED_CICIDS_TEST, TRANSFORMED_NET_TRAIN, TRANSFORMED_NET_TEST\n",
    "    \n",
    "    def preprocess(self, dataframe, is_test = False):\n",
    "        dataframe.fillna(dataframe.mean(), inplace=True)\n",
    "\n",
    "        if is_test == True:\n",
    "            TARGETS = ['binary_class', 'multi_class']\n",
    "            ordinal_encoder = OrdinalEncoder()\n",
    "            for target_column in TARGETS: \n",
    "                dataframe[f'{target_column}'] = ordinal_encoder.fit_transform(dataframe[[f'{target_column}']]).astype('int')        \n",
    "        return dataframe\n",
    "    \n",
    "    def pipeline_preparation(self, df):\n",
    "    \n",
    "        num_var = [i[0] for i in df.dtypes if (((i[1]=='int') | (i[1]=='bigint') | (i[1]=='double')) & (i[0]!='binary_class'))]\n",
    "\n",
    "        label_indexes = StringIndexer(inputCol = 'binary_class', outputCol = 'label', handleInvalid = 'keep')\n",
    "        assembler = VectorAssembler(inputCols = num_var, outputCol = \"features\")\n",
    "        rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "\n",
    "        stages = [assembler, label_indexes, rf]\n",
    "\n",
    "        return stages\n",
    "\n",
    "    def ExtractFeatureImp(self, featureImp, dataset, featuresCol):\n",
    "        list_extract = []\n",
    "        for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "            list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "        varlist = pd.DataFrame(list_extract)\n",
    "        varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "        return(varlist.sort_values('score', ascending = False))\n",
    "    \n",
    "    def save_csv(self, dataset, dataset_name, dataset_type):\n",
    "        DESTINATION = Path('datasets', f'SELECTED_{dataset_name}')\n",
    "        DESTINATION.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        FILE = DESTINATION / f'{dataset_type}.csv'\n",
    "\n",
    "        dataset.to_csv(FILE, index=False)\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\n",
      "Hi! Would u like to see available datasets to process? Y/n: Y\n",
      "['CICIDS', 'NET', 'All']\n",
      "Good! Choose one of datasets to work with pasting name shown in previous step: NET\n",
      "Nice, now we will preprocess the data\n",
      "The data is prepared, start doing Feature Selection\n",
      "We know important features, transform the data\n",
      "Saving selected data\n",
      "You can check your new data!\n"
     ]
    }
   ],
   "source": [
    "SparkFactory().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pd():\n",
    "    TRANSFORMED_CICIDS_TRAIN = pd.read_csv('datasets/TRANSFORMED_CICIDS/train.csv')\n",
    "    TRANSFORMED_CICIDS_TEST = pd.read_csv('datasets/TRANSFORMED_CICIDS/test.csv')\n",
    "\n",
    "    TRANSFORMED_NET_TRAIN = pd.read_csv('datasets/TRANSFORMED_NET/train.csv')\n",
    "    TRANSFORMED_NET_TEST = pd.read_csv('datasets/TRANSFORMED_NET/test.csv')\n",
    "    \n",
    "    return TRANSFORMED_CICIDS_TRAIN, TRANSFORMED_CICIDS_TEST, TRANSFORMED_NET_TRAIN, TRANSFORMED_NET_TEST\n",
    "\n",
    "TRANSFORMED_CICIDS_TRAIN, TRANSFORMED_CICIDS_TEST, TRANSFORMED_NET_TRAIN, TRANSFORMED_NET_TEST = read_pd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace NaN to mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataframe, is_test = False):\n",
    "    dataframe.fillna(dataframe.mean(), inplace=True)\n",
    "    \n",
    "    if is_test == True:\n",
    "        TARGETS = ['binary_class', 'multi_class']\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "        for target_column in TARGETS: \n",
    "            dataframe[f'{target_column}'] = ordinal_encoder.fit_transform(dataframe[[f'{target_column}']]).astype('int')        \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "pd_cicids_train = preprocess(TRANSFORMED_CICIDS_TRAIN, True)\n",
    "pd_cicids_test = preprocess(TRANSFORMED_CICIDS_TEST)\n",
    "\n",
    "pd_net_train = preprocess(TRANSFORMED_NET_TRAIN, True)\n",
    "pd_net_test = preprocess(TRANSFORMED_NET_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns_cicids = pd_cicids_train[['binary_class', 'multi_class']]\n",
    "target_columns_net = pd_net_train[['binary_class', 'multi_class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(__name__).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read DataFrames to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CICIDS\n",
    "pd_cicids_train = spark.createDataFrame(pd_cicids_train)\n",
    "# pd_cicids_test = spark.createDataFrame(pd_cicids_test)\n",
    "\n",
    "#NET\n",
    "pd_net_train = spark.createDataFrame(pd_net_train)\n",
    "# pd_net_test = spark.createDataFrame(pd_net_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop one target to fit Feature Importance model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['multi_class']\n",
    "\n",
    "pd_cicids_train = pd_cicids_train.select([column for column in pd_cicids_train.columns if column not in drop_list])\n",
    "pd_net_train = pd_net_train.select([column for column in pd_net_train.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Feature Importance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_preparation(df):\n",
    "    \n",
    "    num_var = [i[0] for i in df.dtypes if (((i[1]=='int') | (i[1]=='bigint') | (i[1]=='double')) & (i[0]!='binary_class'))]\n",
    "\n",
    "    label_indexes = StringIndexer(inputCol = 'binary_class', outputCol = 'label', handleInvalid = 'keep')\n",
    "    assembler = VectorAssembler(inputCols = num_var, outputCol = \"features\")\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "\n",
    "    stages = [assembler, label_indexes, rf]\n",
    "    \n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pipelines\n",
    "\n",
    "cicids_pipeline =  Pipeline(stages = pipeline_preparation(pd_cicids_train))\n",
    "net_pipeline    =  Pipeline(stages = pipeline_preparation(pd_net_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipelines\n",
    "\n",
    "cicids_model = cicids_pipeline.fit(pd_cicids_train)\n",
    "net_model    = net_pipeline.fit(pd_net_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform DataFrames\n",
    "\n",
    "pd_cicids_train_ = cicids_model.transform(pd_cicids_train)\n",
    "pd_net_train_ = net_model.transform(pd_net_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check feature importance vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(37, {0: 0.0027, 1: 0.0101, 2: 0.1089, 3: 0.0952, 4: 0.0041, 5: 0.1645, 6: 0.0011, 7: 0.0096, 8: 0.0004, 9: 0.0086, 10: 0.0003, 11: 0.0898, 12: 0.018, 13: 0.0106, 15: 0.0, 17: 0.0116, 18: 0.028, 19: 0.0002, 20: 0.0055, 21: 0.065, 22: 0.093, 23: 0.0791, 24: 0.0583, 25: 0.0763, 27: 0.008, 28: 0.0025, 29: 0.0055, 30: 0.0014, 32: 0.0021, 33: 0.0031, 34: 0.0, 35: 0.0059, 36: 0.0305})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cicids_model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(37, {0: 0.0078, 2: 0.0501, 3: 0.1213, 4: 0.0032, 5: 0.0003, 6: 0.018, 7: 0.1068, 8: 0.0208, 9: 0.0246, 10: 0.1439, 11: 0.1664, 12: 0.0035, 13: 0.0011, 14: 0.0071, 15: 0.0041, 16: 0.0341, 17: 0.0437, 18: 0.0119, 19: 0.003, 20: 0.0201, 21: 0.001, 22: 0.0401, 23: 0.0, 28: 0.004, 29: 0.0009, 31: 0.0313, 32: 0.0035, 33: 0.0194, 35: 0.0191, 36: 0.0889})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist_cicids = ExtractFeatureImp(cicids_model.stages[-1].featureImportances, pd_cicids_train_, \"features\")\n",
    "varlist_net = ExtractFeatureImp(net_model.stages[-1].featureImportances, pd_net_train_, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "varidx_cicids = [x for x in varlist_cicids['idx'][0:20]]\n",
    "varidxt_net= [x for x in varlist_net['idx'][0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer_cicids = VectorSlicer(inputCol=\"features\", outputCol=\"features2\", indices=varidx_cicids)\n",
    "selected_cicids_train_ = slicer_cicids.transform(pd_cicids_train_)\n",
    "\n",
    "slicer_net = VectorSlicer(inputCol=\"features\", outputCol=\"features2\", indices=varidxt_net)\n",
    "selected_net_train_ = slicer_net.transform(pd_net_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_features_cicids = list(ExtractFeatureImp(cicids_model.stages[-1].featureImportances, pd_cicids_train_, \"features\").head(20).name)\n",
    "import_features_net = list(ExtractFeatureImp(net_model.stages[-1].featureImportances, pd_net_train_, \"features\").head(20).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_selected_cicids_train = selected_cicids_train_.toPandas()\n",
    "pd_selected_net_train = selected_net_train_.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_selected_cicids_train = pd_selected_cicids_train[import_features_cicids]\n",
    "pd_selected_net_train = pd_selected_net_train[import_features_net]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(dataset, dataset_name, dataset_type):\n",
    "    DESTINATION = Path('datasets', f'SELECTED_{dataset_name}')\n",
    "    DESTINATION.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    FILE = DESTINATION / f'{dataset_type}.csv'\n",
    "    \n",
    "    dataset.to_csv(FILE, index=False)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCAT LABELS AND DATASETS\n",
    "train_cicids = pd.concat([pd_selected_cicids_train, target_columns_cicids], axis = 1)\n",
    "test_cicids = pd_cicids_test[import_features_cicids]\n",
    "\n",
    "train_net = pd.concat([pd_selected_net_train, target_columns_net], axis = 1)\n",
    "test_net = pd_net_test[import_features_net]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_csv(dataset=train_cicids, dataset_name='CICIDS', dataset_type='train')\n",
    "save_csv(dataset=test_cicids, dataset_name='CICIDS', dataset_type='test')\n",
    "\n",
    "save_csv(dataset=train_net, dataset_name='NET', dataset_type='train')\n",
    "save_csv(dataset=test_net, dataset_name='NET', dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS_CICIDS = [\n",
    "                       'sa',\n",
    "                       'da',\n",
    "                       'http_content_type',\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (12,21,25,34,39,41,43,45,48,50,52,53,56,59,61) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "def read_pd():\n",
    "    CICIDS_TRAIN = pd.read_csv('datasets/CICIDS/train.csv')\n",
    "    \n",
    "    return CICIDS_TRAIN\n",
    "\n",
    "CICIDS_TRAIN = read_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CICIDS_TRAIN = CICIDS_TRAIN[CATEGORICAL_COLUMNS_CICIDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CICIDS_TRAIN = CICIDS_TRAIN.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_cicids = spark.createDataFrame(CICIDS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['multi_class']\n",
    "\n",
    "pd_cicids = pd_cicids.select([column for column in pd_cicids.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding and assembling\n",
    "encoding_var = [i[0] for i in pd_cicids.dtypes if (i[1]=='string') & (i[0]!='binary_class')]\n",
    "\n",
    "string_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep') for c in encoding_var]\n",
    "label_indexes = StringIndexer(inputCol = 'binary_class', outputCol = 'label', handleInvalid = 'keep')\n",
    "assembler = VectorAssembler(inputCols = encoding_var, outputCol = \"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed = 8464,\n",
    "                            numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "\n",
    "pipe = Pipeline(stages = string_indexes + [assembler, label_indexes, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicer_cicids = ChiSqSelector(numTopFeatures=20, featuresCol=\"features\", outputCol=\"features2\", labelCol=\"binary_class\")\n",
    "# selected_cicids_train_ = slicer_cicids.fit(pd_cicids).transform(pd_cicids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 1 features selected\n",
      "+---+------------------+-------+----------------+\n",
      "| id|          features|clicked|selectedFeatures|\n",
      "+---+------------------+-------+----------------+\n",
      "|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n",
      "|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n",
      "|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n",
      "+---+------------------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "result = selector.fit(df).transform(df)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
